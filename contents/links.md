## Useful links

### Summary

- [Deep learning - intro](#deeplearning)
- [Neural network](#neuralnetwork)
- [Logistic regression](#logisticregression)
- [Loss function](#lossfunction)
- [Gradient descent](#gradientdescent)
- [Activation function](#activationfunction)
- [Backpropagation](#backpropagation)
- [Hyperparameters](#hyperparameters)
- [Bias vs Variance](#biasvsvariance)
- [Regularization](#regularization)
- [Dropout](#dropout)
- [Overfittinng](#overfitting)
- [Mini-batch](#minibatch)
- [RMSprop](#rmsprop)
- [Optimization](#optimization)

#### Neural network
- [Build your first neural network - Keras](https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/)
- [Medium: How to build your own neural network from scratch in Python](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6)

- [Medium: Simple neural networks in Python](https://towardsdatascience.com/inroduction-to-neural-networks-in-python-7e0b422e6c24)

- [Medium: ANN Pima India Diabetes](https://medium.com/@randerson112358/build-your-own-artificial-neural-network-using-python-f37d16be06bf)

#### Deep learning
- [Introdution Deep learning](https://www.sas.com/pt_br/insights/analytics/deep-learning.html)

#### Logistic regression
- [Medium: Logistic regression detailed overview](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)

- [Interpretatable ml - logistic regression](https://christophm.github.io/interpretable-ml-book/logistic.html)

#### Loss function
- [Intro loss functions](https://algorithmia.com/blog/introduction-to-loss-functions)

- [Common loss functions](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23)

- [how to choose loss functions](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)

#### Gradient descent

- [Gradient checking](https://towardsdatascience.com/coding-neural-network-gradient-checking-5222544ccc64)

#### Activation function

- [Softmax](https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d)

- [Activation functions](https://medium.com/@himanshuxd/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e)

#### Backpropagation

- [Forward propagation and backpropagation](https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76)

#### Hyperparameters

#### Bias vs Variance

#### Regularization

#### Dropout

#### Overfitting

- [Reduce overfitting](https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e)

#### Mini-batch

- [Mini batch vs batch](https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a)

- [Intro mini batch](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)

#### RMSprop

- [RMSprop](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a)

- [Gradient desc and RMSprop](https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b)

#### Optimization

- [Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)
